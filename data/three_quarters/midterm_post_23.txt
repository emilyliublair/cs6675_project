Question: 
    FS Ordering and Atomicity ClarificationHeya, sorry this is kind of long, I just wanted to clear up a few ambiguous things from lecture:In lecture it was discussed that 3 MUST happen before 4,5 for ordering reasons and I just wanted to clarify the reasons for that because there was a confusing answer to a read-after-write question at the end of lecture.Since the goal of shadowing is to enable crash consistency, we don't need to flush after 3 before copying in 4 because there's a chance we get out of date data right? Rather, it's to ensure that the disk has persisted the shadow bit flip to the block written by 1,2 before altering the "original" block in 4,5?To clarify, we don't need this ordering to have up to date data for 4,5 (because those changes, including the shadow bit, are stored in RAM/buffer), but rather to ensure that we don't overwrite the "original" block (the block initially copied in 1) before changing the shadow bit to activate the safe block which happens in 3? Thus in the event of a crash, we aren't pointing to a block that's being currently overwritten by 4,5?I guess a follow-up to that is this is only needed when there are 2 consecutive writes to the same (logical? since technically there are two physical) block right? And these events happen/are important when they are being persisted to disk? So, why not just completely avoid this issue and combine the alterations made in 5 with 2? Functionally, blocks in the buffer don't care about shadow bits. So, on persist, it would look logically like {1, 2, 5} -> {3}; crash consistency is not violated, and you get a nice performance benefit because you don't have to do unnecessary copying and flushing.One thing that I just thought of for the above is maybe it was meant to display that all writes need to be ordered, even if they are on different blocks, since shadowing doesn't provide ordering like log does? So, it would prevent, for example, breaking isolation. In that case, the explanation and example is a bit confusing.Also, group commits were kind of glossed over, so I just wanted to ask: these only apply to same-block, "adjacent" commits, right? So, if we had a log ordered {e1, e2, ... ei}, we could merge e1, e2, and e3 if they alter the same block, but not e5 if there was an e4 altering a different block since that would violate valid prefix ordering? And, they have to be part of the same block because log entries are themselves one block?Or what do group commits actually do? Maybe I have a misunderstanding of how the log is structured, but from what I understand, we have some log header which holds information about the log entries and their checksums, followed by 512B blocks, each of which is a log entry. So are group commits instead saying let's write multiple entries consecutively and then go back to the header and write their checksums? Or is it saying let's have one checksum for many blocks, in which case how is the logging metadata stored?midterm

    Student Answer: 
    

    Instructor Answer: 
    

    Followup Discussions:
    @714_f1Sebastian Tyler Morgenstern5 days agoThanks to whoever replied, but I'm pretty sure that's not the case (maybe I'm wrong though!). Consider that each dirent is 16 bytes (14 bytes for the name + 2 bytes for the inode num). If there are 13, that is 16*13=208B which is well over what's allocated for inodes (64B).I think instead directories have a pointer to a data block on disk which stores the array of dirents? It wasn't immediately clear to me looking over the slides and in lecture that's why I was a bit confused. Let me know if anything there is wrongI found this image (and a few like it) that seem to support this:Note that the first data block of the directory inode points to the dirent block1Anonymous Comp5 days agoYou are right, looking back at the code it definitely seems like the dirents are being read from disk. Tracing calls from some function like dirlookup that uses the dirents takes you through readi, bread, bmap. Looking through that code right now is helping me piece together the dirent access, and yeah it is eventually getting a data block location from the inode’s addrs array.1Sebastian Tyler Morgenstern5 days agoCool! Let me know if you find anything about why it's capped at 13, I just want to make sure I'm not missing anything like extra overhead or something that might be limiting it0Anonymous Comp5 days agoActually, I don’t think anything explicitly limits the number of dirents in a directory other than just the maximum file size / the size of dirent struct. this would lead to about 4000 possible dirents in a directory. This is reflected in the for-loop conditions of the screenshot below which is fromdirlookupinfs.c. It seems like we can change the size of the directory similarly to the size of other files, and access data in the same way, so long as we recognize that data will be in the form of dirent structs.In short I think the slide might be wrong… This would be worth bringing up in class for sure.0Sebastian Tyler Morgenstern5 days agoYeah that's what I was confused about... I think instead of 4000 it'd be closer to 384 considering there are normally 12 pointers to data blocks in a normal inode, so 12*32, where 32 is the number of dirents you could fit in a 512B block. I guess if we really needed a lot of files in a directory and wanted to keep the inode structure we could get like (12+128)*32 by using an indirect pointer.The way it seems to be outlined is it just uses the first direct pointer to a block though, so I was just asking here to verify that this is how dirents are stored AND was wondering if there was a design decision or reason why xv6 caps at 13, since 13 (12+1) is also (confusingly) the number of address pointer attributes dedicated in a file inode structure, so it'd be really easy to confuse a dirent as being stored as a part of the directory inode instead of in a block pointed to by the dir inode.0
    