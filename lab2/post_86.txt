Question: 
    Looping Over All Page DirectoriesI noticed this caveat in the lab2 instructions:"You may not waste excess memory unnecessarily, or perform particularly computationally inefficient activities (like scanning all page tables of all processes on page fault)."And I was wondering if this was referring to checking every pagedirectoryto check if any other processes are referencing a faulting page.If so, I'm wondering why this is considered computationally inefficient. Sure, it's a lot of memory acccesses, but since it's a fixed size (64) and occurs on some page faults which I imagine are already somewhat rare, that seems like a fair tradeoff for having almost 0 additional memory overhead. I also can't think of a better way to do this, so if this is disallowed then I would appreciate any pointers in the right direction.lab2

    Student Answer: 
    

    Instructor Answer: 
    And I was wondering if this was referring to checking every pagedirectoryto check if any other processes are referencing a faulting page.Both page tables and directories. The README just uses “page table” to keep things simple.If so, I’m wondering why this is considered computationally inefficient. Sure, it’s a lot of memory acccesses, but since it’s a fixed size (64) and occurs on some page faults which I imagine are already somewhat rareThe number of processes is essentially arbitrary; xv6 should be able to support any amount, and we wouldn’t want increasing the number of processes to affect the runtime cost of a page fault. Also, 64 is a pretty low number; most kernels support thousands or millions. In a real world scenario we definitely wouldn’t want that many memory accesses on every page fault, especially because a few may happen on every fork! (And although we don’t ask for it in this lab, in a real world scenario we may wish for processes to share pages at different virtual addresses—think shared libraries, memory mapped files, etc. Without a lot of special cases for every way in which processes could share physical pages, this would make the runtime complexity orders of magnitude worse. There is a solution that support this use case just as well and avoids this problem entirely.)You’re correct about the low memory overhead, but you can still get away with a pretty small amount of overhead and minimal runtime cost (i.e., not much more than the cost of copying a single page). Keep in mind that you can dynamically allocate whatever data structures you need to use; the best solution I know of costs at minimum just a few kilobytes of memory and at maximum about 0.1% of the total memory size.For general advice on how to approach this problem without having to look at every process’s page tables, you will need to keep track of some state in order to determine, on any page fault, whether or not you need to copy the page, and this state will likely be per physical page. Given the possible events where a page mapping is added and removed, what is the smallest amount of state you need to store for each physical page to do this?

    Followup Discussions:
    
    